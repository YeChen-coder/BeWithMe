# 本地电子男人搭建日志

全本地造电子男人的日志，列出了过程中踩过的各种坑和最终（by now）的解决方案。给诸位朋友共勉。

本地电子人，主动说话的那种，不用打字唤醒。没有固定的人物限制，图片语音提示词啥的都能换。全都是站在大佬的肩膀上，把一串儿伟大的开源项目串起来了。

如果要细说里面其实有很多很多能再做优化的东西，但是作者明天要回国，暂别亲爱的显卡，所以短期内没法动手。流程这里其实写的很清楚了，代码文件还要研究研究写写注释再放 GitHub。

结果案例：`./VideoUsedInDocx/Wav2LipHarveyResult.mp4`
或者我的小红书账号，

---

## 关于 LLM

最开始用的 `openai-4o-mini`，因为帧序列需要作为输入，所以其实很费 token。我这边估算了下，50 轮（每次都是 one-round），16 分钟，token 数：3,552,300 Tokens，成本为 0.54 刀。按这个推算，一小时就要花费 2.16 美刀，也就是 15 元左右人民币（加上税）。其实也可以接受，但是我真不喜欢看着大把 token 花出去，也不喜欢被网络制约，所以就去探索了本地 LLM 解决方案。

我这里的设备是 **4070 super 12GB，内存 32GB**。之前有成功本地部署过 GPT-20B 搭 RAG 做推理，但是其实那时候就很勉强了，能跑，但是设备频繁 OOM 很难正常使用（虽然简历上还是得写 production level 就是了）。所以在找的时候就会关注能处理视频 + 小一些的模型。

### Qwen3-vl:4b

`Qwen3-vl:4b` 本身是 VL 版本，根据文档是有能输入视频的能力的——本质仍然是模型把视频拆成帧序列去处理，但是我这里用的是 Ollama 版本，所以不支持直接输入视频，只能输入多张图片来模拟视频输入。

`Qwen3-vl:4b` 用下来有两个主要问题：

**第一**，仍然是资源限制，我这里最开始每轮放进去 5 张图片，图片能剪就剪只保留中央的方块，但是加上 prompt 后，仍然会花比较长的时间做 processing，甚至已经到下一轮了（图片是定时拍的，ollama 在后台 run model，所以 inference 的时候是在给本地 request）要开新一轮的 inference 上一轮也没有结果。这点后来就是改成 3 张图片，quality 设定为 low 去勉强解决。这样的制约也导致了我没有再去尝试类似 8b 的更复杂也更强的模型。

**第二**，qwen3 是自带 thinking 的，所以有时候会疯狂 thinking 说车轱辘话但是就是不给结果。这种没办法修，只能直接用 `num_predict`（我这里是 ollama generate 获取 stream 输出）截断，遇到这种情况这一轮就废掉了。我最开始还想做设置让程序不要频繁说话打扰用户，但是现在看起来它自己就给自己限制住了。

---

## 关于 Text->音频 的踩坑记录

音频最开始用 `gpt-4o` 的 TTS，要费 token 且不能训练所以很快就改了。

### Edge-tts

微软的 TTS，声音很好听且免费，但是还是需要联网且不支持训练。

### Bark TTS

这个的好处是语调非常真实，坏处是乱说话。它很"诡异"，creepy 的那种，它说的话和输入的 text 根本就不对应。查到根本原因可能和它自身的模型、缓存机制都有关系。但是它又很真实，导致大半夜听男女老少尖叫叹气各种语调的 "Hello, my name is Suno. And, uh — and I like pizza." 的我非常无助。

（这个适合恐怖场景配音，真的，我要是机器人，起义那天我就装个这个来做节目效果。）

### GPT-SoVITS-v3lora-20250228（最终选择）

最终的音频生成选的是 **GPT-SoVITS-v3lora-20250228**，自己找的音频训练的，非常好用，训得巨快，但是请注意一定要用 GitHub 的代码，因为它的下载包里有些代码文件没更新，会出 bug，而这些问题都在 GitHub 里被解决了。所以请务必从 GitHub pull 下来最新的代码文件。

而且，它的环境配置需要注意一下。我这里其他东西都是配置在同一个 conda 环境下，只有这个因为包的冲突很严重所以给它新开了一个 `GPTSoVits` 环境。

---

## 用于音频->视频的踩坑选项

### Wav2Lip

效果其实现在看是可以接受的，但是它的问题是要么只动嘴巴，其他部分焊死不动（`./VideoUsedInDocx/Wav2LipHarveyResult.mp4`）。如果扩大嘴巴的设定范围，则视频中会让脖子和身体产生诡异的分离。

Wav2lip 在我这里的硬件环境只能跑图生视频，如果用视频（`./VideoUsedInDocx/HarveyRefer.mp4`）作为 reference，哪怕已经把参考视频画面剪得不能再小了，甚至只有三秒，也会宕机（这里指的是完全看不到进度）。

### Wan2GP

非常厉害的框架，输出巨好（`./VideoUsedInDocx/Wan2GPHarveyResult.mp4`），图生视频，人物肩膀能自然晃动，脸上表情非常真实——除了它用了一个小时来生成 3s 的视频。但是 again，这是我的硬件环境的限制，不是框架的问题。

### SadTalker（最终选择）

到目前看起来最合适的就是 **SadTalker**，图生视频，出来的效果可以接受，速度也还可以，对 5s 内语音纯推理 60-70s 内能出来（这里指的是已经载入到 GPU 后，其实前面载入模型还是要花一些处理时间，总体 2min 是 expected）。

从这个项目的介绍，它主要关注三次元人脸，我试了秦彻的图片（3D 模型的类真人图）出来效果能接受，但是像是钟离那样的纯二次元估计费劲了。以及，下一阶段要做 Harvey Spector（真人）的搭建，到时候看结果再更新。

---

## 自动播放视频

`Video_player.py` 负责在桌面上保持一个窗口，等 SadTalker 生成视频后自动播放。但这里还是有问题，对手机/电脑，如果之前没有音频播放，那么开启播放到听到声音是有个时间差的，会让前面的几个字被吞掉。对长音频无所谓，但是我这里本来就没几秒就很影响了。

这个问题最后是让 Claude 解决的。（好像是搞了一个时间帧对齐，又让音频服务开着——就自己去看代码吧，反正能跑）

---

## 最终串起来

- **Ollama** 需要在后台跑 `qwen3-vl:4b`
- **GPTSoVits** 开 `api_v2.py` 本地等请求（前文写过 GPTSoVits 和其他的 conda 环境不一样，但是这里反正是本地 http，所以不用担心跨环境使用的问题）
- **SadTalker** 是基于轮询（就是每隔 30s 查一遍有新音频没），随用随开
- **Video_player.py** 负责自动播放新生成的视频